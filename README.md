# ReTA
ğŸ‰  Our paper **"Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations"** has been accepted at **ACM MM 2025**!

ğŸ”—**Link**: [https://arxiv.org/abs/2507.09500](https://arxiv.org/abs/2507.09500)

### ğŸ“š Overview

<p align="center">
  <img src="assets/reta_pipeline.png" alt="ReTA Pipeline Overview" width="820">
</p>

### ğŸ› ï¸ Environment

```bash
conda create -n reta python=3.8 -y
conda activate reta

pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116

pip install -r requirements.txt
```

### ğŸ—‚ï¸ Datasets

Please follow [DATASETS.md](https://github.com/kdiAAA/TDA/blob/main/docs/DATASETS.md) to prepare the datasets and annotations.

Our dataset organization example is as follows:

```
TTA_Data/
â”œâ”€ imagenet/
â”‚  â”œâ”€ classnames.txt
â”‚  â””â”€ images/
â”‚     â””â”€ val/
â”œâ”€ imagenet-adversarial/            
â”‚  â”œâ”€ classnames.txt
â”‚  â””â”€ imagenet-a                             
â”œâ”€ imagenet-rendition/           
â”‚  â”œâ”€ classnames.txt    
â”‚  â””â”€ imagenet-r
â”œâ”€ imagenet-sketch/         
â”‚  â”œâ”€ classnames.txt         
â”‚  â””â”€ images/
â”œâ”€ imagenetv2/
â”‚  â”œâ”€ classnames.txt 
â”‚  â””â”€ imagenetv2-matched-frequency-format-val  


â”œâ”€ caltech-101/
â”‚  â”œâ”€ split_zhou_Caltech101.json
â”‚  â””â”€ 101_ObjectCategories/
â”œâ”€ dtd/
â”‚  â”œâ”€ split_zhou_DescribableTextures.json
â”‚  â”œâ”€ images/
â”‚  â””â”€ labels/                       
â”œâ”€ eurosat/
â”‚  â”œâ”€ split_zhou_EuroSAT.json
â”‚  â””â”€ 2750/
â”œâ”€ fgvc_aircraft/
â”‚  â”œâ”€ images/
â”‚  â””â”€ images_variant_test.txt                       
â”œâ”€ food-101/
â”‚  â”œâ”€ images/
â”‚  â””â”€ split_zhou_Food101.json                         
â”œâ”€ oxford_flowers/
â”‚  â”œâ”€ jpg/
â”‚  â”œâ”€ imagelabels.mat                   
â”‚  â”œâ”€ cat_to_name.json  
â”‚  â””â”€ split_zhou_OxfordFlowers.json      
â”œâ”€ oxford_pets/
â”‚  â”œâ”€ split_zhou_OxfordPets.json
â”‚  â”œâ”€ images/
â”‚  â””â”€ annotations/                  
â”œâ”€ stanford_cars/
â”‚  â”œâ”€ cars_test/
â”‚  â””â”€ split_zhou_StanfordCars.json
â”œâ”€ sun397/
â”‚  â”œâ”€ SUN397/
â”‚  â””â”€ split_zhou_SUN397.json                   
â””â”€ ucf101/
   â”œâ”€ UCF-101-midframes/                       
   â””â”€ split_zhou_UCF101.json                 

```


### ğŸš€ Quick Start

To reproduce the results reported in **Tables 1** and **Table 2** of our paper, run the following commands:

```bash
# OOD benchmark (ViT-B/16)
bash ./scripts/run_ood_benchmark_vit.sh

# OOD benchmark (RN50)
bash ./scripts/run_ood_benchmark_rn50.sh

# Cross-domain benchmark (ViT-B/16)
bash ./scripts/run_cd_benchmark_vit.sh

# Cross-domain benchmark (RN50)
bash ./scripts/run_cd_benchmark_rn50.sh
```
Hyperparameter configurations are in `configs/`. In these config files, `align` corresponds to $\lambda_2$ and train_w corresponds to $\lambda_1$ in Eq. (18). For other hyperparameters, please refer to [TDA](https://github.com/kdiAAA/TDA) and [DPE](https://github.com/zhangce01/DPE-CLIP/tree/main).

### ğŸ¤— Acknowledgements

Our work benefited from the public code and dataset instructions of [TPT](https://github.com/azshue/TPT), [CoOp](https://github.com/KaiyangZhou/CoOp), [TDA](https://github.com/kdiAAA/TDA) and [DPE](https://github.com/zhangce01/DPE-CLIP/tree/main). Thanks to the authors for open-sourcing implementations and providing clear setup guides.

### ğŸ“Œ Citation

If you find this code helpful, kindly consider citing: 

```
@article{liang2025advancing,
  title={Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations},
  author={Liang, Yiwen and Chen, Hui and Xiong, Yizhe and Zhou, Zihan and Lyu, Mengyao and Lin, Zijia and Niu, Shuaicheng and Zhao, Sicheng and Han, Jungong and Ding, Guiguang},
  journal={arXiv preprint arXiv:2507.09500},
  year={2025}
}
```